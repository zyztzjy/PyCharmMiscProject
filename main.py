# soure/main.py
import tempfile
from typing import Dict, List, Optional
import streamlit as st
import pandas as pd
from datetime import datetime
import json
import sys
import os
import yaml
import hashlib
import time

from matplotlib import pyplot as plt
from soure.document.document_processor import DocumentProcessor
from soure.llm.intel_extractor import LLMExtractor
from soure.llm.scenario_config import ScenarioConfig, ScenarioType, ScenarioRule

sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from soure.embedding.vectorizer_qwen import QwenVectorizer
from soure.rag.qwen_rag_processor import QwenRAGProcessor


st.set_page_config(
    page_title="ä¼ä¸šæ™ºèƒ½åˆ†æåŠ©æ‰‹",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

if "analysis_history" not in st.session_state:
    st.session_state.analysis_history = []

if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []

if "message_counter" not in st.session_state:
    st.session_state.message_counter = 0


def generate_unique_key(prefix: str, data: Optional[Dict] = None) -> str:
    """ç”Ÿæˆå”¯ä¸€çš„key"""
    st.session_state.message_counter += 1
    counter = st.session_state.message_counter

    if data:
        timestamp = data.get("timestamp", str(time.time()))
        query = data.get("query", "")
        key_str = f"{prefix}_{timestamp}_{query}_{counter}"
    else:
        key_str = f"{prefix}_{time.time()}_{counter}"

    return f"{prefix}_{hashlib.md5(key_str.encode()).hexdigest()[:8]}"


@st.cache_resource
def init_system():
    """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
    try:
        with open("config/config.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        api_key = "sk-6892cc65b78941e7a6981cae25997c0b"
        if not api_key:
            st.error("è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡æˆ–Streamlit secrets")
            st.stop()

        vectorizer = QwenVectorizer(config)
        rag_processor = QwenRAGProcessor(
            vectorizer=vectorizer,
            api_key=api_key,
            model=config['llm'].get('model', 'qwen-max')
        )

        # åˆå§‹åŒ–å¤§æ¨¡å‹è¯†åˆ«å™¨
        llm_extractor = LLMExtractor(api_key)

        return {
            "config": config,
            "vectorizer": vectorizer,
            "rag_processor": rag_processor,
            "llm_extractor": llm_extractor,  # æ›¿æ¢ä¸ºæ–°çš„è¯†åˆ«å™¨
            "api_key": api_key
        }
    except Exception as e:
        st.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        st.stop()


def add_message(role: str, content: str, analysis_result: Optional[Dict] = None):
    """æ·»åŠ æ¶ˆæ¯åˆ°èŠå¤©å†å²"""
    if analysis_result:
        analysis_result["unique_id"] = generate_unique_key("analysis", {
            "timestamp": analysis_result.get("timestamp", str(datetime.now().timestamp())),
            "query": analysis_result.get("query", "")
        })

    message_id = generate_unique_key("msg", {"content": content, "role": role})

    st.session_state.messages.append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat(),
        "analysis_result": analysis_result,
        "unique_id": message_id
    })

    if analysis_result:
        st.session_state.analysis_history.append(analysis_result)


def display_message(message: Dict):
    """æ˜¾ç¤ºå•æ¡æ¶ˆæ¯"""
    role = message["role"]
    content = message["content"]
    analysis_result = message.get("analysis_result")
    message_id = message.get("unique_id", "")

    with st.chat_message(role):
        st.markdown(content)

        if analysis_result and role == "assistant":
            display_analysis_details(analysis_result, message_id)


def display_scenario_specific_analysis(response: Dict, scenario_rule: ScenarioRule, company_code: str):
    """æ ¹æ®åœºæ™¯æ˜¾ç¤ºç‰¹å®šçš„åˆ†æå†…å®¹"""

    # æ˜¾ç¤ºåœºæ™¯æ ‡é¢˜å’Œæ¡†æ¶
    st.subheader(f"{scenario_rule.icon} {scenario_rule.display_name}")
    st.caption(f"åˆ†ææ¡†æ¶: {scenario_rule.framework}")

    # åˆ›å»ºåœºæ™¯ç‰¹å®šç»„ä»¶
    create_scenario_specific_components(scenario_rule, response, company_code)


    # æ˜¾ç¤ºè¯¦ç»†åˆ†æå†…å®¹
    st.subheader("è¯¦ç»†åˆ†æå†…å®¹")

    # æœ¬åœ°æ–‡æ¡£åˆ†æ
    detailed_analysis = response.get("detailed_analysis", {})
    if detailed_analysis and isinstance(detailed_analysis, dict):
        local_based = detailed_analysis.get("local_based", [])
        if local_based and isinstance(local_based, list):
            with st.expander("åŸºäºæœ¬åœ°æ–‡æ¡£çš„åˆ†æ", expanded=False):
                for i, item in enumerate(local_based):
                    if isinstance(item, str):
                        st.markdown(f"**{i + 1}.** {item}")

        # ç½‘ç»œä¿¡æ¯åˆ†æ
        web_based = detailed_analysis.get("web_based", [])
        if web_based and isinstance(web_based, list):
            with st.expander("åŸºäºç½‘ç»œä¿¡æ¯çš„åˆ†æ", expanded=False):
                for i, item in enumerate(web_based):
                    if isinstance(item, str):
                        st.markdown(f"**{i + 1}.** {item}")

        # ç»¼åˆåˆ†æ
        integrated = detailed_analysis.get("integrated", [])
        if integrated and isinstance(integrated, list):
            with st.expander("ç»¼åˆåˆ†æç»“è®º", expanded=False):
                for i, item in enumerate(integrated):
                    if isinstance(item, str):
                        st.markdown(f"**{i + 1}.** {item}")
    else:
        analysis_list = response.get("analysis", [])
        if analysis_list and isinstance(analysis_list, list):
            with st.expander("åˆ†æè¦ç‚¹", expanded=False):
                for i, item in enumerate(analysis_list):
                    if isinstance(item, str):
                        st.markdown(f"**{i + 1}.** {item}")
        else:
            st.info("æš‚æ— è¯¦ç»†åˆ†æå†…å®¹")


def display_analysis_details(result: Dict, message_id: str = ""):
    """æ˜¾ç¤ºåˆ†æç»“æœçš„è¯¦ç»†ä¿¡æ¯"""
    if not isinstance(result, dict):
        st.warning("åˆ†æç»“æœæ ¼å¼å¼‚å¸¸")
        return

    # ç”Ÿæˆå”¯ä¸€key
    unique_key = result.get("unique_id", message_id)
    if not unique_key:
        unique_key = generate_unique_key("analysis", result)

    response = result.get("response", {})
    retrieval_stats = result.get("retrieval_stats", {})
    scenario_name = result.get("scenario_name", "è‡ªå®šä¹‰åˆ†æ")
    company_code = result.get("company_code", "æœªè¯†åˆ«åˆ°ä¼ä¸š")

    # è·å–åœºæ™¯è§„åˆ™
    scenario_rule = ScenarioConfig.get_scenario_by_name(scenario_name)
    if not scenario_rule:
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°åœºæ™¯ï¼Œä½¿ç”¨æ’¤å¦ä¼ä¸šåˆ†æä½œä¸ºé»˜è®¤
        scenario_rule = ScenarioConfig.get_default_scenario()
        scenario_name = scenario_rule.display_name

    # åˆ›å»ºå¯æŠ˜å çš„è¯¦ç»†ä¿¡æ¯åŒºåŸŸ
    with st.expander("æŸ¥çœ‹åˆ†æè¯¦æƒ…", expanded=False):
        if response and isinstance(response, dict):
            display_scenario_specific_analysis(response, scenario_rule, company_code)

            st.divider()

            # ä¼ä¸šé£é™©æç¤º
            st.subheader("ä¼ä¸šé£é™©æç¤º")
            risk_assessment = response.get("risk_assessment", {})

            if risk_assessment and isinstance(risk_assessment, dict):
                identified_risks = risk_assessment.get("identified_risks", [])
                risk_level = risk_assessment.get("risk_level", "æœªçŸ¥")
                rationale = risk_assessment.get("rationale", "")

                col1, col2 = st.columns([1, 3])
                with col1:
                    risk_color = {
                        "é«˜": "ğŸ”´",
                        "ä¸­": "ğŸŸ¡",
                        "ä½": "ğŸŸ¢",
                        "æœªçŸ¥": "âšª"
                    }
                    risk_icon = risk_color.get(risk_level, "âšª")
                    st.metric("é£é™©çº§åˆ«", f"{risk_icon} {risk_level}")

                with col2:
                    if rationale:
                        st.caption(f"é£é™©è¯„ä¼°ä¾æ®: {rationale}")

                if identified_risks and isinstance(identified_risks, list):
                    with st.expander(f"å…·ä½“é£é™©ç‚¹ ({len(identified_risks)}ä¸ª)", expanded=True):
                        for i, risk in enumerate(identified_risks):
                            if isinstance(risk, str):
                                st.warning(f"**â€¢ é£é™©{i + 1}:** {risk}")
                else:
                    st.info("æš‚æ— å…·ä½“é£é™©ä¿¡æ¯")
            else:
                risks_list = response.get("risks", [])
                if risks_list and isinstance(risks_list, list):
                    for i, risk in enumerate(risks_list):
                        if isinstance(risk, str):
                            st.warning(f"**â€¢ é£é™©{i + 1}:** {risk}")
                else:
                    st.info("æš‚æ— é£é™©æç¤º")

        st.divider()

        # å‚è€ƒæ–‡æ¡£
        source_docs = result.get("source_documents", [])
        if source_docs and isinstance(source_docs, list):
            with st.expander(f"å‚è€ƒæ–‡æ¡£ ({len(source_docs)}ä¸ª)", expanded=False):
                for i, doc in enumerate(source_docs):
                    doc_title = "æœªçŸ¥æ–‡æ¡£"
                    doc_content = "æ— å†…å®¹é¢„è§ˆ"
                    doc_source = "æœªçŸ¥æ¥æº"
                    doc_original_filename = "æœªçŸ¥æ–‡ä»¶"

                    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿docæ˜¯å­—å…¸ç±»å‹æ‰è°ƒç”¨getæ–¹æ³•
                    if isinstance(doc, dict):
                        doc_title = doc.get('title', doc.get('source', 'æœªçŸ¥æ–‡æ¡£'))
                        doc_content = doc.get('content_preview', doc.get('content', 'æ— é¢„è§ˆå†…å®¹'))
                        doc_source = doc.get('source', 'æœªçŸ¥æ¥æº')
                        
                        # å°è¯•è·å–åŸå§‹æ–‡ä»¶å
                        metadata = doc.get('metadata', {})
                        if isinstance(metadata, dict):
                            doc_original_filename = metadata.get('original_filename', 
                                                               metadata.get('file_name', 
                                                                           metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')))
                        else:
                            doc_original_filename = doc.get('original_filename', 
                                                                  doc.get('file_name', 
                                                                         doc.get('source', 'æœªçŸ¥æ–‡ä»¶')))
                    elif isinstance(doc, str):
                        doc_title = f"æ–‡æ¡£ {i + 1}"
                        doc_content = doc[:200] + "..." if len(doc) > 200 else doc
                        doc_source = "æ–‡æœ¬å†…å®¹"
                        doc_original_filename = "æœªçŸ¥æ–‡ä»¶"

                    with st.expander(f"{doc_title}", expanded=False):
                        st.caption(f"**æ¥æº:** {doc_source}")
                        st.caption(f"**åŸå§‹æ–‡ä»¶:** {doc_original_filename}")
                        st.write(doc_content)
        else:
            st.info("æš‚æ— å‚è€ƒæ–‡æ¡£")

        # æ“ä½œæŒ‰é’®
        st.divider()
        col1, col2 = st.columns(2)
        with col1:
            export_key = f"export_{unique_key}"
            if st.button("å¯¼å‡ºæŠ¥å‘Š", key=export_key, width='stretch'):
                report = {
                    "ç”Ÿæˆæ—¶é—´": result.get("timestamp", datetime.now().isoformat()),
                    "åˆ†æåœºæ™¯": scenario_rule.display_name,
                    "ç›®æ ‡ä¼ä¸š": result.get("company_code"),
                    "æŸ¥è¯¢å†…å®¹": result.get("query"),
                    "åˆ†æç»“æœ": result.get("response", {}),
                    "æ£€ç´¢ç»Ÿè®¡": result.get("retrieval_stats", {}),
                    "å‚è€ƒæ–‡æ¡£": result.get("source_documents", [])
                }
                st.download_button(
                    label="ä¸‹è½½JSONæŠ¥å‘Š",
                    data=json.dumps(report, ensure_ascii=False, indent=2),
                    file_name=f"ä¼ä¸šåˆ†ææŠ¥å‘Š_{company_code or 'æœªçŸ¥ä¼ä¸š'}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    key=f"download_{unique_key}"
                )


def clear_chat_history():
    """æ¸…ç©ºèŠå¤©å†å²"""
    st.session_state.messages = []
    st.session_state.analysis_history = []
    st.success("èŠå¤©è®°å½•å·²æ¸…ç©º")


def create_scenario_specific_components(scenario_rule: ScenarioRule, response: Dict, company_code: str):
    """åˆ›å»ºåœºæ™¯ç‰¹å®šçš„å±•ç¤ºç»„ä»¶"""

    scenario_name = scenario_rule.display_name

    # æ ¹æ®åœºæ™¯åç§°é€‰æ‹©ç»„ä»¶
    if scenario_name == "æ’¤å¦ä¼ä¸šåˆ†æ":
        create_withdrawal_analysis_components(response, scenario_rule, company_code)
    elif scenario_name == "é•¿æœŸè¾…å¯¼ä¼ä¸šåˆ†æ":
        create_tutoring_analysis_components(response, scenario_rule, company_code)
    elif scenario_name == "ä¸Šä¸‹æ¸¸ä¼ä¸šåˆ†æ":
        create_relationship_analysis_components(response, scenario_rule, company_code)


def create_withdrawal_analysis_components(response: Dict, scenario_rule: ScenarioRule, company_code: str):
    """åˆ›å»ºæ’¤å¦ä¼ä¸šåˆ†æä¸“ç”¨ç»„ä»¶"""

    # 1. æ’¤å¦é£é™©ä»ªè¡¨ç›˜
    st.subheader("æ’¤å¦é£é™©ç»¼åˆè¯„ä¼°")

    risk_assessment = response.get("risk_assessment", {})
    withdrawal_analysis = response.get("withdrawal_analysis", {})

    col1, col2, col3 = st.columns(3)
    with col1:
        risk_level = risk_assessment.get("risk_level", "æœªçŸ¥")
        risk_config = {
            "é«˜": {"color": "ğŸ”´", "desc": "å­˜åœ¨é‡å¤§å®¡æ ¸éšœç¢"},
            "ä¸­": {"color": "ğŸŸ¡", "desc": "éƒ¨åˆ†é—®é¢˜éœ€é‡ç‚¹æ•´æ”¹"},
            "ä½": {"color": "ğŸŸ¢", "desc": "é—®é¢˜ç›¸å¯¹å¯æ§"},
            "æœªçŸ¥": {"color": "âšª", "desc": "ä¿¡æ¯ä¸è¶³æ— æ³•è¯„ä¼°"}
        }
        config = risk_config.get(risk_level, risk_config["æœªçŸ¥"])
        st.metric("æ’¤å¦é£é™©ç­‰çº§", f"{config['color']} {risk_level}")
        st.caption(config['desc'])

    with col2:
        risks = risk_assessment.get("identified_risks", [])
        if isinstance(risks, list):
            st.metric("ä¸»è¦é—®é¢˜æ•°é‡", len(risks))
        else:
            st.metric("ä¸»è¦é—®é¢˜æ•°é‡", 0)


    # 2. æ’¤å¦åŸå› æ—¶é—´çº¿
    st.subheader("æ’¤å¦å…³é”®äº‹ä»¶æ—¶é—´çº¿")

    timeline_data = withdrawal_analysis.get("timeline", [])
    if timeline_data and isinstance(timeline_data, list):
        for event in timeline_data:
            if isinstance(event, dict):
                with st.expander(f"{event.get('date', 'æœªçŸ¥æ—¥æœŸ')} - {event.get('event', 'æœªçŸ¥äº‹ä»¶')}"):
                    st.write(f"**äº‹ä»¶ç±»å‹**: {event.get('type', 'æœªçŸ¥')}")
                    st.write(f"**å½±å“ç¨‹åº¦**: {event.get('impact', 'æœªçŸ¥')}")
                    if event.get('description'):
                        st.write(f"**è¯¦ç»†æè¿°**: {event['description']}")
            elif isinstance(event, str):
                st.info(f"â€¢ {event}")
    else:
        st.info("æš‚æ— è¯¦ç»†æ—¶é—´çº¿ä¿¡æ¯")

    # 3. å®¡æ ¸é—®è¯¢é‡ç‚¹åˆ†æ
    st.subheader("å®¡æ ¸é—®è¯¢é‡ç‚¹åˆ†æ")

    inquiry_analysis = withdrawal_analysis.get("inquiry_analysis", {})
    if inquiry_analysis and isinstance(inquiry_analysis, dict):
        rounds = inquiry_analysis.get("inquiry_rounds", [])

        if rounds and isinstance(rounds, list):
            for i, round_data in enumerate(rounds):
                if isinstance(round_data, dict):
                    with st.expander(f"ç¬¬{round_data.get('round_number', i + 1)}è½®é—®è¯¢ ({round_data.get('date', '')})",
                                     expanded=i == 0):
                        col_a, col_b = st.columns(2)
                        with col_a:
                            st.write(f"**é—®é¢˜æ•°é‡**: {round_data.get('question_count', 0)}")

                            focus_areas = round_data.get('focus_areas', [])
                            if isinstance(focus_areas, list):
                                st.write(f"**é‡ç‚¹é¢†åŸŸ**: {', '.join(focus_areas)}")
                            else:
                                st.write(f"**é‡ç‚¹é¢†åŸŸ**: {focus_areas}")

                        with col_b:
                            st.write(f"**å›å¤è´¨é‡**: {round_data.get('reply_quality', 'æœªçŸ¥')}")
                            st.write(f"**æ•´æ”¹æƒ…å†µ**: {round_data.get('rectification', 'æœªçŸ¥')}")

                        key_questions = round_data.get('key_questions', [])
                        if key_questions and isinstance(key_questions, list):
                            st.write("**å…³é”®é—®é¢˜**:")
                            for q in key_questions:
                                if isinstance(q, str):
                                    st.write(f"â€¢ {q}")
                elif isinstance(round_data, str):
                    st.info(f"â€¢ é—®è¯¢è½®æ¬¡ {i + 1}: {round_data}")
    else:
        st.info("æš‚æ— å®¡æ ¸é—®è¯¢åˆ†ææ•°æ®")

    # 4. æ•´æ”¹å»ºè®®ä¸é‡æ–°ä¸Šå¸‚è·¯å¾„
    st.subheader("æ•´æ”¹å»ºè®®ä¸é‡æ–°ä¸Šå¸‚è·¯å¾„")

    recommendations = response.get("recommendations", [])
    if recommendations and isinstance(recommendations, list):
        tab1, tab2, tab3 = st.tabs(["ç«‹å³æ•´æ”¹é¡¹", "ä¸­æœŸæ”¹è¿›é¡¹", "é•¿æœŸä¼˜åŒ–é¡¹"])

        with tab1:
            urgent_items = [r for r in recommendations if isinstance(r, dict) and r.get('priority') == 'urgent']
            if urgent_items:
                for item in urgent_items:
                    st.warning(f"**{item.get('title', '')}**")
                    st.write(item.get('description', ''))
                    st.write(f"*é¢„è®¡è€—æ—¶: {item.get('duration', 'æœªçŸ¥')}*")
            else:
                st.info("æ— ç«‹å³æ•´æ”¹é¡¹")

        with tab2:
            medium_items = [r for r in recommendations if isinstance(r, dict) and r.get('priority') == 'medium']
            if medium_items:
                for item in medium_items:
                    st.info(f"**{item.get('title', '')}**")
                    st.write(item.get('description', ''))
                    st.write(f"*é¢„è®¡è€—æ—¶: {item.get('duration', 'æœªçŸ¥')}*")
            else:
                st.info("æ— ä¸­æœŸæ”¹è¿›é¡¹")

        with tab3:
            long_items = [r for r in recommendations if isinstance(r, dict) and r.get('priority') == 'long']
            if long_items:
                for item in long_items:
                    st.success(f"**{item.get('title', '')}**")
                    st.write(item.get('description', ''))
                    st.write(f"*é¢„è®¡è€—æ—¶: {item.get('duration', 'æœªçŸ¥')}*")
            else:
                st.info("æ— é•¿æœŸä¼˜åŒ–é¡¹")
    else:
        st.info("æš‚æ— æ•´æ”¹å»ºè®®")
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        st.write("**è°ƒè¯•ä¿¡æ¯**: ")
        st.write(f"- recommendationså­—æ®µç±»å‹: {type(recommendations)}")
        st.write(f"- recommendationså†…å®¹: {recommendations}")
        # æ˜¾ç¤ºæ•´ä¸ªresponseç»“æ„çš„ä¸€éƒ¨åˆ†ç”¨äºè°ƒè¯•
        st.write(f"- responseä¸­åŒ…å«çš„é”®: {list(response.keys())}")


# ä¿®æ”¹ create_tutoring_analysis_components å‡½æ•°ä¸­çš„ç›¸å…³éƒ¨åˆ†ï¼š

def create_tutoring_analysis_components(response: Dict, scenario_rule: ScenarioRule, company_code: str):
    """åˆ›å»ºé•¿æœŸè¾…å¯¼ä¼ä¸šåˆ†æä¸“ç”¨ç»„ä»¶"""

    # 1. è¾…å¯¼å†ç¨‹æ¦‚è§ˆ
    st.subheader("è¾…å¯¼å†ç¨‹æ¦‚è§ˆ")

    tutoring_analysis = response.get("tutoring_analysis", {})

    col1, col2, col3 = st.columns(3)
    with col1:
        start_date = tutoring_analysis.get('start_date', 'æœªçŸ¥')
        st.metric("è¾…å¯¼å¼€å§‹æ—¶é—´", start_date)

    with col2:
        duration = tutoring_analysis.get('duration_months', 0)
        st.metric("è¾…å¯¼æ—¶é•¿(æœˆ)", duration)

    with col3:
        stage = tutoring_analysis.get('current_stage', 'æœªçŸ¥')
        st.metric("å½“å‰é˜¶æ®µ", stage)

    # 2. è¾…å¯¼é˜¶æ®µæ—¶é—´çº¿
    st.subheader("è¾…å¯¼é˜¶æ®µåˆ†æ")

    stages = tutoring_analysis.get('stages', [])
    if stages and isinstance(stages, list):
        for stage in stages:
            if isinstance(stage, dict):
                status_icon = "âœ…" if stage.get('completed') else "â³"
                with st.expander(f"{status_icon} {stage.get('name', 'æœªçŸ¥é˜¶æ®µ')} ({stage.get('date_range', '')})"):
                    st.write(f"**ä¸»è¦å†…å®¹**: {stage.get('content', '')}")
                    st.write(f"**å®Œæˆæƒ…å†µ**: {'å·²å®Œæˆ' if stage.get('completed') else 'è¿›è¡Œä¸­/æœªå¼€å§‹'}")

                    issues = stage.get('issues', [])
                    if issues and isinstance(issues, list):
                        st.write("**å­˜åœ¨é—®é¢˜**:")
                        for issue in issues:
                            if isinstance(issue, str):
                                st.warning(f"â€¢ {issue}")
            elif isinstance(stage, str):
                st.info(f"â€¢ {stage}")
    else:
        st.info("æš‚æ— è¾…å¯¼é˜¶æ®µä¿¡æ¯")

    # 3. ä¸Šå¸‚éšœç¢åˆ†æ
    st.subheader("ä¸»è¦ä¸Šå¸‚éšœç¢åˆ†æ")

    obstacles = tutoring_analysis.get("ipo_obstacles", [])
    if obstacles and isinstance(obstacles, list):
        obstacle_data = []
        for obs in obstacles:
            if isinstance(obs, dict):
                obstacle_data.append({
                    "éšœç¢ç±»å‹": obs.get('type', ''),
                    "ä¸¥é‡ç¨‹åº¦": obs.get('severity', ''),
                    "å½±å“ç¯èŠ‚": obs.get('impact_stage', ''),
                    "æ•´æ”¹éš¾åº¦": obs.get('rectification_difficulty', '')
                })

        if obstacle_data:
            obstacle_df = pd.DataFrame(obstacle_data)
            st.dataframe(obstacle_df, width='stretch')

            # éšœç¢ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
            severity_counts = obstacle_df['ä¸¥é‡ç¨‹åº¦'].value_counts()
            if not severity_counts.empty:
                fig, ax = plt.subplots()
                severity_counts.plot(kind='bar', ax=ax, color=['#ff6b6b', '#ffa726', '#66bb6a'])
                ax.set_ylabel('éšœç¢æ•°é‡')
                ax.set_title('ä¸Šå¸‚éšœç¢ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
                st.pyplot(fig)
        else:
            st.info("æš‚æ— æœ‰æ•ˆçš„éšœç¢æ•°æ®")
    else:
        st.info("æš‚æ— ä¸Šå¸‚éšœç¢åˆ†æ")

# soure/main.py
# ä¿®æ”¹ create_relationship_analysis_components å‡½æ•°ä¸­çš„ç›¸å…³éƒ¨åˆ†ï¼š

def create_relationship_analysis_components(response: Dict, scenario_rule: ScenarioRule, company_code: str):
    """åˆ›å»ºä¸Šä¸‹æ¸¸ä¼ä¸šåˆ†æä¸“ç”¨ç»„ä»¶"""

    # 1. å…³è”ç½‘ç»œæ¦‚è§ˆ
    st.subheader("ğŸ”— å…³è”ç½‘ç»œæ¦‚è§ˆ")

    relationship_analysis = response.get("relationship_analysis", {})

    col1, col2, col3 = st.columns(3)
    with col1:
        entity_count = relationship_analysis.get('entity_count', 0)
        st.metric("å…³è”å®ä½“æ•°é‡", entity_count)

    with col2:
        relation_count = relationship_analysis.get('relation_count', 0)
        st.metric("å…³è”å…³ç³»æ•°é‡", relation_count)

    with col3:
        core_entities = relationship_analysis.get('core_entities', 0)
        st.metric("æ ¸å¿ƒå…³è”å®ä½“", core_entities)

    # 2. å…³è”å…³ç³»çŸ©é˜µ
    relations = relationship_analysis.get('relations', [])
    if relations:
        relation_data = []
        for rel in relations:
            # å®‰å…¨å¤„ç†ï¼šç¡®ä¿relæ˜¯å­—å…¸
            if isinstance(rel, dict):
                relation_data.append({
                    "å…³è”æ–¹A": rel.get('entity_a', ''),
                    "å…³è”æ–¹B": rel.get('entity_b', ''),
                    "å…³ç³»ç±»å‹": rel.get('relation_type', ''),
                    "äº¤æ˜“é‡‘é¢": rel.get('transaction_amount', 'N/A'),
                    "æ¯”ä¾‹(%)": rel.get('percentage', 'N/A'),
                    "å…¬å…æ€§": rel.get('fairness', 'æœªçŸ¥')
                })
            elif isinstance(rel, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç®€å•æ˜¾ç¤º
                relation_data.append({
                    "å…³è”å…³ç³»": rel
                })

        if relation_data:
            relation_df = pd.DataFrame(relation_data)
            st.dataframe(relation_df, width='stretch')
        else:
            st.info("æš‚æ— æœ‰æ•ˆçš„å…³è”å…³ç³»æ•°æ®")

    # 3. é£é™©ä¼ å¯¼åˆ†æ
    st.subheader("å…³è”é£é™©ä¼ å¯¼åˆ†æ")

    risk_transmission = relationship_analysis.get("risk_transmission_analysis", {})
    if risk_transmission and isinstance(risk_transmission, dict):
        transmission_paths = risk_transmission.get('paths', [])

        if transmission_paths and isinstance(transmission_paths, list):
            for path in transmission_paths:
                if isinstance(path, dict):
                    with st.expander(f"é£é™©ä¼ å¯¼è·¯å¾„: {path.get('from', '')} â†’ {path.get('to', '')}"):
                        st.write(f"**ä¼ å¯¼æœºåˆ¶**: {path.get('mechanism', '')}")
                        st.write(f"**å½±å“ç¨‹åº¦**: {path.get('impact_level', '')}")
                        st.write(f"**å‘ç”Ÿæ¦‚ç‡**: {path.get('probability', '')}")

                        if path.get('mitigation_measures'):
                            st.write("**é˜²èŒƒæªæ–½**:")
                            for measure in path.get('mitigation_measures', []):
                                st.info(f"â€¢ {measure}")
                elif isinstance(path, str):
                    st.info(f"â€¢ {path}")
        else:
            st.info("æš‚æ— é£é™©ä¼ å¯¼è·¯å¾„ä¿¡æ¯")
    else:
        st.info("æš‚æ— é£é™©ä¼ å¯¼åˆ†ææ•°æ®")

    # 4. ç‹¬ç«‹æ€§æ•´æ”¹å»ºè®® - ä¿®å¤è¿™é‡Œçš„é”™è¯¯
    st.subheader("ç‹¬ç«‹æ€§æ•´æ”¹å»ºè®®")

    independence_issues = relationship_analysis.get("independence_issues", [])
    if independence_issues and isinstance(independence_issues, list):
        for i, issue in enumerate(independence_issues):
            if isinstance(issue, dict):
                # ä½¿ç”¨å®‰å…¨çš„getæ–¹æ³•ï¼Œæä¾›é»˜è®¤å€¼
                issue_type = issue.get('type', f'ç‹¬ç«‹æ€§é—®é¢˜{i + 1}')
                severity = issue.get('severity', 'æœªçŸ¥')

                with st.expander(f"{issue_type} - ä¸¥é‡ç¨‹åº¦: {severity}"):
                    st.write(f"**é—®é¢˜æè¿°**: {issue.get('description', '')}")
                    st.write(f"**å½±å“åˆ†æ**: {issue.get('impact_analysis', '')}")

                    suggestions = issue.get('rectification_suggestions', [])
                    if suggestions and isinstance(suggestions, list):
                        st.write("**æ•´æ”¹å»ºè®®**:")
                        for suggestion in suggestions:
                            if isinstance(suggestion, str):
                                st.success(f"â€¢ {suggestion}")
                    elif isinstance(suggestions, str):
                        st.success(f"â€¢ {suggestions}")
            elif isinstance(issue, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
                with st.expander(f"ç‹¬ç«‹æ€§é—®é¢˜ {i + 1}"):
                    st.write(f"**é—®é¢˜**: {issue}")
    else:
        st.info("æš‚æ— ç‹¬ç«‹æ€§æ•´æ”¹å»ºè®®")


# ä¿®æ”¹ search_companies_page å‡½æ•°

def search_companies_page():
    """ä¼ä¸šæ£€ç´¢é¡µé¢"""
    st.title("ä¼ä¸šæ™ºèƒ½æ£€ç´¢ä¸­å¿ƒ")

    # æ£€ç´¢è¡¨å•
    with st.form("search_form"):
        col1, col2 = st.columns([4, 1])

        with col1:
            search_query = st.text_input(
                "è¯·è¾“å…¥æ£€ç´¢æŸ¥è¯¢",
                placeholder="ä¾‹å¦‚ï¼šåˆ—å‡ºå­˜åœ¨æ’¤å¦å¯èƒ½çš„ä¼ä¸šã€æŸ¥æ‰¾é«˜é£é™©è¾…å¯¼ä¼ä¸šã€æ£€ç´¢å…³è”äº¤æ˜“é¢‘ç¹çš„å…¬å¸...",
                help="æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œç³»ç»Ÿä¼šæ™ºèƒ½åˆ†ææ‚¨çš„æ„å›¾",
                key="search_input"
            )

        with col2:
            search_button = st.form_submit_button(
                "æ™ºèƒ½æ£€ç´¢",
                type="primary",
                use_container_width=True
            )

        # é«˜çº§é€‰é¡¹
        with st.expander("âš™ï¸ é«˜çº§é€‰é¡¹", expanded=False):
            col_a, col_b, col_c = st.columns(3)

            with col_a:
                search_intent = st.selectbox(
                    "æ£€ç´¢æ„å›¾",
                    ["è‡ªåŠ¨è¯†åˆ«", "æ’¤å¦ä¼ä¸š(å·²å‘ç”Ÿ)", "æ’¤å¦é£é™©è¯„ä¼°", "è¾…å¯¼ä¼ä¸š", "å…³è”ä¼ä¸š", "é«˜é£é™©ä¼ä¸š", "æ‰€æœ‰ä¼ä¸š"],
                    help="æŒ‡å®šæ£€ç´¢çš„ä¼ä¸šç±»å‹ï¼Œé€‰æ‹©'è‡ªåŠ¨è¯†åˆ«'è®©ç³»ç»Ÿæ™ºèƒ½åˆ¤æ–­"
                )

            with col_b:
                result_limit = st.number_input("æ˜¾ç¤ºæ•°é‡", min_value=5, max_value=50, value=15)

            with col_c:
                use_llm = st.checkbox("ä½¿ç”¨LLMæ™ºèƒ½åˆ†æ", value=True,
                                      help="ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æå’Œä¿¡æ¯æå–")

    # å¤„ç†æ£€ç´¢æŸ¥è¯¢
    if search_button and search_query:
        with st.spinner("æ­£åœ¨æ™ºèƒ½åˆ†æ..."):
            try:
                system = st.session_state.system
                rag_processor = system["rag_processor"]

                # ç¡®å®šæœç´¢æ„å›¾
                if search_intent == "è‡ªåŠ¨è¯†åˆ«":
                    intent = "general"
                elif search_intent == "æ’¤å¦ä¼ä¸š(å·²å‘ç”Ÿ)":
                    intent = "æ’¤å¦ä¼ä¸š"
                elif search_intent == "æ’¤å¦é£é™©è¯„ä¼°":
                    intent = "æ’¤å¦é£é™©è¯„ä¼°"
                else:
                    intent_map = {
                        "æ’¤å¦ä¼ä¸š(å·²å‘ç”Ÿ)": "æ’¤å¦ä¼ä¸š",
                        "æ’¤å¦é£é™©è¯„ä¼°": "æ’¤å¦é£é™©è¯„ä¼°", 
                        "è¾…å¯¼ä¼ä¸š": "è¾…å¯¼ä¼ä¸š",
                        "å…³è”ä¼ä¸š": "å…³è”ä¼ä¸š",
                        "é«˜é£é™©ä¼ä¸š": "é«˜é£é™©",
                        "æ‰€æœ‰ä¼ä¸š": "general"
                    }
                    intent = intent_map.get(search_intent, "general")

                # æ‰§è¡Œæ™ºèƒ½æ£€ç´¢
                search_result = rag_processor.intelligent_company_search(
                    search_query=search_query,
                    search_intent=intent,
                    limit=result_limit,
                    use_llm_analysis=use_llm
                )

                # ä¿å­˜åˆ°session_state
                st.session_state.last_search_result = search_result
                st.session_state.last_search_query = search_query

                # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                display_intelligent_search_results(search_result)

                # ä¿å­˜æ£€ç´¢å†å²
                if "search_history" not in st.session_state:
                    st.session_state.search_history = []

                st.session_state.search_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "query": search_query,
                    "result_count": search_result.get("total_found", 0),
                    "search_method": search_result.get("search_method", "unknown")
                })

            except Exception as e:
                st.error(f"æ£€ç´¢å¤±è´¥: {str(e)}")

    elif search_button and not search_query:
        st.warning("è¯·è¾“å…¥æ£€ç´¢æŸ¥è¯¢")

    # æ˜¾ç¤ºå†å²æ£€ç´¢
    show_search_history()


def display_intelligent_search_results(search_result: Dict):
    """æ˜¾ç¤ºæ™ºèƒ½æ£€ç´¢ç»“æœ"""
    total_found = search_result.get("total_found", 0)
    companies = search_result.get("companies", [])
    search_intent = search_result.get("search_intent", "æœªçŸ¥")
    intent_analysis = search_result.get("intent_analysis", {})

    if total_found == 0:
        st.info("æœªæ‰¾åˆ°ç›¸å…³ä¼ä¸š")
        if search_result.get("message"):
            st.info(search_result["message"])
        return

    # æ˜¾ç¤ºæ£€ç´¢æ¦‚è§ˆ
    st.success(f"ğŸ” æ‰¾åˆ° {total_found} ä¸ªç›¸å…³ä¼ä¸š")


    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = search_result.get("statistics", {})

    if stats:
        st.subheader("ğŸ“Š æ£€ç´¢ç»Ÿè®¡")

        # é£é™©åˆ†å¸ƒ
        risk_dist = stats.get("risk_distribution", {})
        if risk_dist:
            cols = st.columns(4)
            risk_colors = {"é«˜": "ğŸ”´", "ä¸­": "ğŸŸ¡", "ä½": "ğŸŸ¢", "æœªçŸ¥": "âšª"}

            for i, (level, icon) in enumerate(risk_colors.items()):
                count = risk_dist.get(level, 0)
                with cols[i]:
                    st.metric(f"{icon} {level}é£é™©", f"{count}ä¸ª")

        # åœºæ™¯åˆ†å¸ƒ
        scenario_dist = stats.get("scenario_distribution", {})
        if scenario_dist:
            scenario_icons = {"æ’¤å¦": "âš ï¸", "è¾…å¯¼": "ğŸ“…", "å…³è”": "ğŸ”—", "å…¶ä»–": "ğŸ¢"}

            scenario_text = []
            for scenario, count in scenario_dist.items():
                if count > 0:
                    icon = scenario_icons.get(scenario, "ğŸ“Š")
                    scenario_text.append(f"{icon} {scenario}: {count}ä¸ª")

            if scenario_text:
                st.caption(" | ".join(scenario_text))

    st.divider()

    # ä¼ä¸šåˆ—è¡¨
    for idx, company in enumerate(companies):
        with st.container():
            # åˆ›å»ºä¼ä¸šå¡ç‰‡
            create_company_card(company, idx)

            st.divider()


def create_company_card(company: Dict, idx: int):
    """åˆ›å»ºä¼ä¸šä¿¡æ¯å¡ç‰‡"""
    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿companyæ˜¯å­—å…¸ç±»å‹
    if not isinstance(company, dict):
        st.warning(f"ä¼ä¸šä¿¡æ¯æ ¼å¼å¼‚å¸¸: {type(company)}")
        return

    company_name = company.get("company_name", "æœªçŸ¥ä¼ä¸š")
    company_short_name = company.get("company_short_name", company_name)
    company_code = company.get("company_code", "")

    # é£é™©è¯„ä¼°
    risk_assessment = company.get("risk_assessment", {})
    risk_level = risk_assessment.get("level", "æœªçŸ¥")
    risk_icon = {"é«˜": "ğŸ”´", "ä¸­": "ğŸŸ¡", "ä½": "ğŸŸ¢", "æœªçŸ¥": "âšª"}.get(risk_level, "âšª")

    # ç½®ä¿¡åº¦
    confidence = company.get("confidence_score", 0)

    # åˆ›å»ºå¡ç‰‡æ ‡é¢˜
    col1, col2, col3 = st.columns([6, 2, 2])

    with col1:
        # ä¼ä¸šåç§°å’Œä»£ç 
        title_html = f"<h3>{company_name}"
        if company_code:
            title_html += f" <small style='color: #666; font-weight: normal;'>({company_code})</small>"
        title_html += "</h3>"
        st.markdown(title_html, unsafe_allow_html=True)

        # ä¼ä¸šç®€ç§°
        if company_short_name and company_short_name != company_name:
            st.caption(f"ç®€ç§°: {company_short_name}")

    with col2:
        # é£é™©çº§åˆ«
        st.metric("é£é™©çº§åˆ«", f"{risk_icon} {risk_level}")

        # ç½®ä¿¡åº¦
        if confidence > 0:
            st.progress(confidence / 100, text=f"ç½®ä¿¡åº¦: {confidence}%")

    with col3:
        # æ“ä½œæŒ‰é’®
        action_col1, action_col2 = st.columns(2)

        with action_col1:
            if st.button("è¯¦ç»†åˆ†æ", key=f"analyze_{idx}", use_container_width=True):
                # è®¾ç½®è‡ªåŠ¨æŸ¥è¯¢
                st.session_state.auto_query = f"åˆ†æ{company_name}çš„è¯¦ç»†ä¿¡æ¯"
                st.session_state.current_page = "chat"
                st.rerun()

        with action_col2:
            # å¯¼å‡ºä¼ä¸šä¿¡æ¯
            export_data = json.dumps(company, ensure_ascii=False, indent=2)
            st.download_button(
                label="å¯¼å‡º",
                data=export_data,
                file_name=f"{company_name}_ä¿¡æ¯.json",
                mime="application/json",
                key=f"export_{idx}",
                use_container_width=True
            )

    # æ›´å¤šä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
    with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯", expanded=False):
        # é£é™©è¯¦æƒ…
        if risk_assessment.get("types") or risk_assessment.get("evidence"):
            st.write("**é£é™©è¯„ä¼°è¯¦æƒ…**")

            risk_types = risk_assessment.get("types", [])
            if risk_types:
                st.write(f"é£é™©ç±»å‹: {', '.join(risk_types)}")

            risk_evidence = risk_assessment.get("evidence", "")
            if risk_evidence:
                st.write(f"é£é™©ä¾æ®: {risk_evidence}")

        # LLMåˆ†æç»“æœ
        if company.get("risk_details") or company.get("relevance_analysis"):
            st.write("**æ™ºèƒ½åˆ†æç»“æœ**")

            risk_details = company.get("risk_details", {})
            if risk_details:
                st.write(f"é£é™©è¯¦æƒ…: {risk_details}")

            relevance = company.get("relevance_analysis", "")
            if relevance:
                st.write(f"ç›¸å…³æ€§åˆ†æ: {relevance}")

        # æ–‡æ¡£æ¥æº
        source_docs = company.get("source_documents", [])
        document_refs = company.get("document_references", [])

        if source_docs or document_refs:
            st.write("**ä¿¡æ¯æ¥æº**")

            all_sources = source_docs + document_refs
            for i, source in enumerate(all_sources[:3]):  # æ˜¾ç¤ºå‰3ä¸ªæ¥æº
                source_text = "æœªçŸ¥æ–‡æ¡£"
                doc_type = "æœªçŸ¥ç±»å‹"
                
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿sourceæ˜¯å­—å…¸ç±»å‹æ‰è°ƒç”¨getæ–¹æ³•
                if isinstance(source, dict):
                    source_text = source.get("source", "æœªçŸ¥æ–‡æ¡£")
                    doc_type = source.get("document_type", "æœªçŸ¥ç±»å‹")
                elif isinstance(source, str):
                    source_text = source

                st.caption(f"{i + 1}. {source_text} ({doc_type})")

                snippet = ""
                if isinstance(source, dict):
                    snippet = source.get("content_snippet", "")
                if snippet:
                    st.text(snippet[:200] + "..." if len(snippet) > 200 else snippet)


def show_search_history():
    """æ˜¾ç¤ºæ£€ç´¢å†å²"""
    if "search_history" in st.session_state and st.session_state.search_history:
        with st.expander("ğŸ“‹ æ£€ç´¢å†å²è®°å½•", expanded=False):
            for idx, record in enumerate(reversed(st.session_state.search_history[-5:])):
                timestamp = record['timestamp'][:19]
                query = record['query']
                count = record['result_count']
                method = record.get('search_method', 'æœªçŸ¥')

                # åˆ›å»ºå†å²è®°å½•æ¡ç›®
                col1, col2, col3 = st.columns([3, 1, 2])

                with col1:
                    st.write(f"**{query}**")

                with col2:
                    st.write(f"ğŸ“Š {count}ä¸ªç»“æœ")

                with col3:
                    if st.button(f"é‡æ–°æœç´¢", key=f"re_search_{idx}",
                                 help=f"é‡æ–°æ‰§è¡ŒæŸ¥è¯¢: {query}"):
                        st.session_state.search_input = query
                        st.rerun()

                st.caption(f"{timestamp} | æ–¹æ³•: {method}")


def main():
    # ========== ä¾§è¾¹æ  ==========
    with st.sidebar:
        st.title("âš™ï¸ å¯¼èˆª")

        # é¡µé¢é€‰æ‹©
        page_options = {
            "èŠå¤©åˆ†æ": "chat",
            "ä¼ä¸šæ£€ç´¢": "search"
        }

        selected_page = st.selectbox(
            "é€‰æ‹©åŠŸèƒ½",
            list(page_options.keys()),
            index=0
        )

        page_key = page_options[selected_page]

        # æ–‡æ¡£ç®¡ç†å¸¸é©»æ˜¾ç¤º
        st.subheader("ä¸Šä¼ ä¼ä¸šæ–‡æ¡£")

        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–‡æ¡£",
            type=['pdf', 'docx', 'doc', 'xlsx', 'xls'],
            accept_multiple_files=True,
            help="ä¸Šä¼ ä¼ä¸šç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒPDFã€Wordã€Excelï¼‰ï¼Œæ”¯æŒå¤šæ–‡ä»¶ä¸Šä¼ ",
            key="doc_uploader"
        )

        if uploaded_files:
            st.session_state.uploaded_files = uploaded_files
            
            # ç«‹å³å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£ï¼ˆå¦‚æœç³»ç»Ÿå·²åˆå§‹åŒ–ï¼‰
            if "system" in st.session_state:
                system = st.session_state.system
                vectorizer = system["vectorizer"]
                doc_processor = DocumentProcessor(system["config"])
                
                # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡æ¡£
                processed_count = 0

                for uploaded_file in st.session_state.uploaded_files:
                    temp_dir = tempfile.mkdtemp()
                    temp_path = os.path.join(temp_dir, uploaded_file.name)

                    try:
                        # éªŒè¯æ–‡ä»¶æ‰©å±•å
                        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                        allowed_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls']
                        if file_extension not in allowed_extensions:
                            st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ï¼Œè·³è¿‡å¤„ç†")
                            continue

                        with open(temp_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                        # æå–æ–‡æœ¬å†…å®¹
                        chunks = doc_processor.extract_text_from_document(temp_path)

                        if chunks:
                            # æ˜¾ç¤ºæå–çš„ä¿¡æ¯
                            st.info(f"ä»æ–‡ä»¶ {uploaded_file.name} ä¸­æå–äº† {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                            
                            # æ ¹æ®æ–‡ä»¶åæˆ–å†…å®¹æ¨æ–­æ–‡æ¡£ç±»å‹
                            file_name_lower = uploaded_file.name.lower()
                            document_type = "æœªçŸ¥æ–‡æ¡£"
                            content_lower = ""
                            
                            # è·å–ç¬¬ä¸€å—å†…å®¹ç”¨äºåˆ†æ
                            if chunks and isinstance(chunks[0], dict) and 'content' in chunks[0]:
                                content_lower = chunks[0]['content'].lower()
                            
                            # æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹åˆ¤æ–­æ–‡æ¡£ç±»å‹
                            if "æ’¤å¦" in file_name_lower or "æ’¤å¦" in content_lower:
                                document_type = "æ’¤å¦ä¼ä¸šåˆ—è¡¨"
                            elif "è¾…å¯¼" in file_name_lower or "è¾…å¯¼" in content_lower:
                                document_type = "è¾…å¯¼ä¼ä¸šåˆ—è¡¨"
                            elif "å…³è”" in file_name_lower or "å…³è”" in content_lower:
                                document_type = "å…³è”ä¼ä¸šåˆ—è¡¨"
                            elif "é£é™©" in file_name_lower or "é£é™©" in content_lower:
                                document_type = "é£é™©ä¼ä¸šåˆ—è¡¨"
                            elif "ä¼ä¸šåå•" in file_name_lower or "ä¼ä¸šåå•" in content_lower:
                                document_type = "ä¼ä¸šåå•"
                            elif "ä¼ä¸šåˆ—è¡¨" in file_name_lower or "ä¼ä¸šåˆ—è¡¨" in content_lower:
                                document_type = "ä¼ä¸šåå•"
                            else:
                                document_type = "æŠ¥å‘Šæ–‡æ¡£"
                            
                            # ä¸ºæ¯ä¸ªchunkæ·»åŠ æ–‡æ¡£ç±»å‹ä¿¡æ¯
                            for chunk in chunks:
                                # ç¡®ä¿chunkæ˜¯å­—å…¸ç±»å‹æ‰æ·»åŠ å…ƒæ•°æ®
                                if isinstance(chunk, dict):
                                    if 'metadata' not in chunk:
                                        chunk['metadata'] = {}
                                    chunk['metadata']['document_type'] = document_type
                                    chunk['metadata']['source'] = uploaded_file.name
                                    
                                    # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
                                    chunk['metadata']['upload_time'] = datetime.now().isoformat()
                                    chunk['metadata']['original_filename'] = uploaded_file.name

                            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                            success_count = vectorizer.store_documents(chunks)
                            if success_count:
                                processed_count += success_count
                                st.success(f"æˆåŠŸå­˜å‚¨ {success_count} ä¸ªç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£ç±»å‹: {document_type}")
                            else:
                                st.warning("å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥")
                        else:
                            st.warning(f"æ–‡ä»¶ {uploaded_file.name} ä¸­æ²¡æœ‰æå–åˆ°å†…å®¹")

                    except Exception as e:
                        st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            if os.path.exists(temp_path):
                                os.remove(temp_path)
                            os.rmdir(temp_dir)
                        except:
                            pass
                if processed_count > 0:
                    st.success(f"âœ… å…±å¤„ç†å¹¶å­˜å‚¨äº† {processed_count} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                    # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
                    stats = vectorizer.get_collection_stats()
                    st.info(f"å‘é‡æ•°æ®åº“å½“å‰æœ‰ {stats.get('total_documents', 0)} ä¸ªæ–‡æ¡£")

            with st.expander(f"å·²ä¸Šä¼  ({len(uploaded_files)}ä¸ªæ–‡ä»¶)", expanded=False):
                for idx, file in enumerate(uploaded_files):
                    file_size_mb = file.size / (1024 * 1024)
                    st.write(f"{idx + 1}. **{file.name}** ({file_size_mb:.2f} MB)")

        st.divider()

        # æ“ä½œæŒ‰é’®
        st.subheader("ğŸ› ï¸ æ“ä½œ")
        if st.button("æ¸…ç©ºå¯¹è¯å†å²", width='stretch', help="æ¸…é™¤æ‰€æœ‰èŠå¤©è®°å½•"):
            clear_chat_history()

        if st.button("æ¸…ç©ºå‘é‡åº“", width='stretch', help="æ¸…ç©ºå‘é‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£", type="secondary"):
            with st.spinner("æ­£åœ¨æ¸…ç©ºå‘é‡åº“..."):
                try:
                    system = st.session_state.system
                    vectorizer = system["vectorizer"]
                    
                    # æ¸…ç©ºå‘é‡åº“
                    success = vectorizer.clear_collection()
                    
                    if success:
                        st.success("âœ… å‘é‡åº“å·²æ¸…ç©º")
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        stats = vectorizer.get_collection_stats()
                        st.info(f"å‘é‡æ•°æ®åº“å½“å‰æœ‰ {stats.get('total_documents', 0)} ä¸ªæ–‡æ¡£")
                    else:
                        st.error("âŒ æ¸…ç©ºå‘é‡åº“å¤±è´¥")
                except Exception as e:
                    st.error(f"æ¸…ç©ºå‘é‡åº“æ—¶å‡ºé”™: {e}")

    # ========== åˆå§‹åŒ–ç³»ç»Ÿ ==========
    if "system" not in st.session_state:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ..."):
            st.session_state.system = init_system()

    # ========== é¡µé¢è·¯ç”± ==========
    if page_key == "search":
        search_companies_page()
    else:

        # ========== ä¸»èŠå¤©ç•Œé¢ ==========
        # æ˜¾ç¤ºèŠå¤©å†å²
        chat_container = st.container()

        with chat_container:
            # æ˜¾ç¤ºæ‰€æœ‰æ¶ˆæ¯
            for idx, message in enumerate(st.session_state.messages):
                display_message(message)

            # å¦‚æœè¿˜æ²¡æœ‰æ¶ˆæ¯ï¼Œæ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
            if not st.session_state.messages:
                st.markdown("""
                <div style='text-align: center; padding: 2rem; color: #666;'>
                    <h3>ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ä¼ä¸šæ™ºèƒ½åˆ†æåŠ©æ‰‹</h3>
                    <p>ğŸ’¡ ç¤ºä¾‹ï¼š</p>
                    <p>â€¢ "åˆ†ææ¬£å¼ºç”µå­(300745)çš„æ’¤å¦åŸå› "</p>
                    <p>â€¢ "è¯„ä¼°æŸç§‘æŠ€å…¬å¸é•¿æœŸè¾…å¯¼çš„ä¸Šå¸‚éšœç¢"</p>
                    <p>â€¢ "åˆ†ææŸé›†å›¢ä¸Šä¸‹æ¸¸å…³è”å…³ç³»"</p>
                </div>
                """, unsafe_allow_html=True)

        # ========== è¾“å…¥åŒºåŸŸ ==========
        input_container = st.container()

        with input_container:
            st.divider()

            # åˆ›å»ºè¾“å…¥è¡¨å•
            with st.form(key="chat_input_form", clear_on_submit=True):
                col1, col2 = st.columns([5, 1])

                with col1:
                    prompt = st.text_area(
                        "è¾“å…¥æ‚¨çš„é—®é¢˜",
                        height=80,
                        placeholder="ä¾‹å¦‚ï¼šåˆ†ææ¬£å¼ºç”µå­(300745)çš„æ’¤å¦åŸå› ã€è¯„ä¼°æŸå…¬å¸çš„ä¸Šå¸‚å¯è¡Œæ€§ã€äº†è§£è¡Œä¸šæœ€æ–°è¶‹åŠ¿ç­‰...",
                        key="chat_input",
                        label_visibility="collapsed",
                        value=st.session_state.get("auto_query", "")
                    )

                with col2:
                    submit_button = st.form_submit_button(
                        "å‘é€",
                        type="primary",
                        width='stretch'
                    )

                if "auto_query" in st.session_state:
                    del st.session_state.auto_query

        # ========== å¤„ç†ç”¨æˆ·è¾“å…¥ ==========
        if submit_button and prompt:
            add_message("user", prompt)
            st.rerun()

        elif submit_button and not prompt:
            st.warning("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")

        # ========== å¤„ç†AIå“åº” ==========
        if (st.session_state.messages and
                st.session_state.messages[-1]["role"] == "user" and
                not hasattr(st.session_state, "processing_message")):

            user_message = st.session_state.messages[-1]["content"]
            st.session_state.processing_message = True

            with chat_container:
                with st.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨åˆ†æ..."):
                        try:
                            # è·å–ç³»ç»Ÿå®ä¾‹
                            system = st.session_state.system
                            rag_processor = system["rag_processor"]
                            vectorizer = system["vectorizer"]
                            llm_extractor = system["llm_extractor"]  # è·å–å¤§æ¨¡å‹è¯†åˆ«å™¨

                            # ä½¿ç”¨å¤§æ¨¡å‹æ™ºèƒ½æå–åœºæ™¯å’Œä¼ä¸šä¿¡æ¯
                            extracted_info = llm_extractor.extract_company_and_scenario(user_message)

                            company_code = extracted_info["company_code"]
                            company_name = extracted_info["company_name"]
                            scenario_type = extracted_info["scenario"]
                            scenario_name = extracted_info["scenario_name"]

                            # è·å–åœºæ™¯è§„åˆ™
                            scenario_rule = ScenarioConfig.get_scenario_rule(scenario_type)

                            # ç¡®ä¿åœºæ™¯åç§°ä¸åœºæ™¯è§„åˆ™åŒ¹é…
                            if scenario_rule:
                                scenario_name = scenario_rule.display_name

                            # æ˜¾ç¤ºæå–çš„ä¿¡æ¯
                            info_text = []
                            if company_name:
                                confidence = extracted_info["confidence"]["company"]
                                info_text.append(f"è¯†åˆ«åˆ°ä¼ä¸š: {company_name} (ç½®ä¿¡åº¦: {confidence:.0%})")
                                if company_code:
                                    info_text[-1] += f" [ä»£ç : {company_code}]"
                            elif company_code:
                                confidence = extracted_info["confidence"]["company"]
                                info_text.append(f"è¯†åˆ«åˆ°ä¼ä¸šä»£ç : {company_code} (ç½®ä¿¡åº¦: {confidence:.0%})")

                            if scenario_name != "è‡ªå®šä¹‰åˆ†æ":
                                confidence = extracted_info["confidence"]["scenario"]
                                info_text.append(f"è¯†åˆ«åˆ°åœºæ™¯: {scenario_name} (ç½®ä¿¡åº¦: {confidence:.0%})")

                            # ä½¿ç”¨ä¼ä¸šåç§°è¿›è¡Œæœç´¢ï¼ˆä¼˜å…ˆä½¿ç”¨åç§°ï¼Œå…¶æ¬¡ä½¿ç”¨ä»£ç ï¼‰
                            search_company = company_name or company_code

                            # å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£ï¼ˆæ”¯æŒPDFã€Wordã€Excelï¼‰
                            if st.session_state.uploaded_files:
                                doc_processor = DocumentProcessor(system["config"])
                                processed_count = 0

                                for uploaded_file in st.session_state.uploaded_files:
                                    temp_dir = tempfile.mkdtemp()
                                    temp_path = os.path.join(temp_dir, uploaded_file.name)

                                    try:
                                        # éªŒè¯æ–‡ä»¶æ‰©å±•å
                                        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                                        allowed_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls']
                                        if file_extension not in allowed_extensions:
                                            st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ï¼Œè·³è¿‡å¤„ç†")
                                            continue

                                        with open(temp_path, "wb") as f:
                                            f.write(uploaded_file.getbuffer())

                                        # æå–æ–‡æœ¬å†…å®¹
                                        chunks = doc_processor.extract_text_from_document(temp_path)

                                        if chunks:
                                            # æ˜¾ç¤ºæå–çš„ä¿¡æ¯
                                            st.info(f"ä»æ–‡ä»¶ {uploaded_file.name} ä¸­æå–äº† {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                                            
                                            # æ ¹æ®æ–‡ä»¶åæˆ–å†…å®¹æ¨æ–­æ–‡æ¡£ç±»å‹
                                            file_name_lower = uploaded_file.name.lower()
                                            document_type = "æœªçŸ¥æ–‡æ¡£"
                                            content_lower = ""
                                            
                                            # è·å–ç¬¬ä¸€å—å†…å®¹ç”¨äºåˆ†æ
                                            if chunks and isinstance(chunks[0], dict) and 'content' in chunks[0]:
                                                content_lower = chunks[0]['content'].lower()
                                            
                                            # æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹åˆ¤æ–­æ–‡æ¡£ç±»å‹
                                            if "æ’¤å¦" in file_name_lower or "æ’¤å¦" in content_lower:
                                                document_type = "æ’¤å¦ä¼ä¸šåˆ—è¡¨"
                                            elif "è¾…å¯¼" in file_name_lower or "è¾…å¯¼" in content_lower:
                                                document_type = "è¾…å¯¼ä¼ä¸šåˆ—è¡¨"
                                            elif "å…³è”" in file_name_lower or "å…³è”" in content_lower:
                                                document_type = "å…³è”ä¼ä¸šåˆ—è¡¨"
                                            elif "é£é™©" in file_name_lower or "é£é™©" in content_lower:
                                                document_type = "é£é™©ä¼ä¸šåˆ—è¡¨"
                                            elif "ä¼ä¸šåå•" in file_name_lower or "ä¼ä¸šåå•" in content_lower:
                                                document_type = "ä¼ä¸šåå•"
                                            elif "ä¼ä¸šåˆ—è¡¨" in file_name_lower or "ä¼ä¸šåˆ—è¡¨" in content_lower:
                                                document_type = "ä¼ä¸šåå•"
                                            else:
                                                document_type = "æŠ¥å‘Šæ–‡æ¡£"
                                            
                                            # ä¸ºæ¯ä¸ªchunkæ·»åŠ æ–‡æ¡£ç±»å‹ä¿¡æ¯
                                            for chunk in chunks:
                                                if 'metadata' not in chunk:
                                                    chunk['metadata'] = {}
                                                chunk['metadata']['document_type'] = document_type
                                                chunk['metadata']['source'] = uploaded_file.name
                                                
                                            # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
                                            chunk['metadata']['upload_time'] = datetime.now().isoformat()
                                            chunk['metadata']['original_filename'] = uploaded_file.name

                                            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                                            success_count = vectorizer.store_documents(chunks)
                                            if success_count:
                                                processed_count += success_count
                                                st.success(f"æˆåŠŸå­˜å‚¨ {success_count} ä¸ªç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£ç±»å‹: {document_type}")
                                            else:
                                                st.warning("å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥")
                                        else:
                                            st.warning(f"æ–‡ä»¶ {uploaded_file.name} ä¸­æ²¡æœ‰æå–åˆ°å†…å®¹")

                                    except Exception as e:
                                        st.error(f"å¤„ç†æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")
                                        import traceback
                                        traceback.print_exc()
                                    finally:
                                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                        try:
                                            if os.path.exists(temp_path):
                                                os.remove(temp_path)
                                            os.rmdir(temp_dir)
                                        except:
                                            pass

                                if processed_count > 0:
                                    st.success(f"âœ… å…±å¤„ç†å¹¶å­˜å‚¨äº† {processed_count} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

                                    # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
                                    stats = vectorizer.get_collection_stats()
                                    st.info(f"å‘é‡æ•°æ®åº“å½“å‰æœ‰ {stats.get('total_documents', 0)} ä¸ªæ–‡æ¡£")

                            # æ‰§è¡Œåˆ†æ
                            result = rag_processor.process_query(
                                query=user_message,
                                scenario=scenario_rule.display_name if scenario_rule else (
                                    scenario_name if scenario_name != "è‡ªå®šä¹‰åˆ†æ" else None),
                                company_code=search_company,
                                use_web_data="auto",
                                scenario_rule=scenario_rule
                            )

                            # ç¡®ä¿resultæ˜¯å­—å…¸
                            if not isinstance(result, dict):
                                result = {
                                    "response": {
                                        "summary": str(result) if result else "åˆ†æç»“æœä¸ºç©º",
                                        "detailed_analysis": {
                                            "local_based": ["æœ¬åœ°æ–‡æ¡£åˆ†æ"],
                                            "web_based": ["ç½‘ç»œä¿¡æ¯åˆ†æ"],
                                            "integrated": ["ç»¼åˆåˆ†æ"]
                                        },
                                        "key_findings": ["åˆ†æå®Œæˆ"],
                                        "risk_assessment": {
                                            "identified_risks": [],
                                            "risk_level": "æœªçŸ¥",
                                            "rationale": "åˆ†æå®Œæˆ"
                                        },
                                        "recommendations": []
                                    },
                                    "retrieval_stats": {},
                                    "source_documents": [],
                                    "query": user_message,
                                    "scenario_name": scenario_name,
                                    "company_code": company_code,
                                    "timestamp": datetime.now().isoformat()
                                }

                            # è·å–å“åº”å†…å®¹
                            response = result.get("response", {})
                            if not isinstance(response, dict):
                                response = {"summary": str(response)}

                            answer = response.get("summary") or "\n".join(response.get("analysis", []))

                            if not answer:
                                answer = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ›´å…·ä½“çš„é—®é¢˜æˆ–ä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚"

                            # æ·»åŠ AIæ¶ˆæ¯åˆ°å†å²
                            add_message("assistant", answer, result)

                        except Exception as e:
                            error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
                            add_message("assistant", error_msg)
                            st.error(f"è¯¦ç»†é”™è¯¯: {e}")

                        finally:
                            if "processing_message" in st.session_state:
                                del st.session_state.processing_message
                            st.rerun()


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªåŠ¨æœç´¢æŸ¥è¯¢
    if "auto_search" in st.session_state:
        # ä¿å­˜æœç´¢æŸ¥è¯¢
        search_query = st.session_state.auto_search
        del st.session_state.auto_search

        # è®¾ç½®é¡µé¢çŠ¶æ€ä¸ºæœç´¢
        st.session_state.current_page = "search"

        # åœ¨æœç´¢é¡µé¢ä¸­è®¾ç½®æŸ¥è¯¢
        if "search_page_initialized" not in st.session_state:
            st.session_state.search_page_initialized = True
            st.session_state.initial_search_query = search_query

        # æ‰§è¡Œä¸»å‡½æ•°
        main()
    else:
        # æ­£å¸¸æ‰§è¡Œ
        main()